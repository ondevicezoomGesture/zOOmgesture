<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>An On-device, In-air Zoom Gesture Interaction for Frugal Head Mounted
Devices</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheet/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheet/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Zoom Gesture</h1>
      <h2 class="project-tagline">An On-device, In-air Zoom Gesture Interaction for Frugal Head Mounted
Devices</h2>
      
     
    </section>

    <section class="main-content">
      <h3>
<a id="Welcome-to-ZoomGesture" class="anchor" href="#welcome-to-telepresence-roi" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Natural zoom Gesture</h3>

<p align ="justify">
  With the resurgence of Head Mounted Displays(HMDs),
in-air gestures form a natural and an intuitive user in-
terface. HMDs such as Microsoft Hololens, Daqri smart
glasses have on-board processors with additional sensors,
making the device expensive. To this end, with the in-
tent to reach mass-market, we extend the gesture interac-
tion space around mobile devices for Augmented/Virtual re-
ality using a frugal video-see-through head-mounts. Our
proposed technique for Hand segmentation and subsequent
zoom gesture classification algorithm utilize only RGB cam-
era in an off-the-shelf mobile device and perform in real-
time. The novelty of our work lies in coming up with (a) a
filtering technique that we term, Multi Orientation Matched
filter(MOMF). MOMF is used for hand segmentation even
with skin-like background in real-time (b) Our zoom gesture
algorithm runs on a resource-constrained android smart-
phones. For evaluation of hand segmentation, compar-
isons of hand segmentation with the existing methods is pre-
sented. For zoom gesture classification, we also demon-
strate that the proposed method outperforms in term of
computational time and yields comparable results in terms
of accuracy with the state-of-the-art SegNet [2] and Mask
RCNN [9]. Further, we conducted user studies on the com-
fort, ease and responsiveness of our proposed algorithm on
Google Cardboard.</p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

<p><img src="https://github.com/ondevicezoomGesture/zOOmgesture/blob/master/img/flowdiagramnew.jpg?raw=true?" alt="Proposed Method " height = "600" width="550" align = "middle"></p>

<p align="justify"> We  present an  approach  for  marker-less  and  real-time touch-less gestures for Zoom in/out on wearables in FPV by tracking
  the movement of Thumb and Index finger. Our  approach  is  particularly  suitable  as  most  of  the smartphones available in 
  the market are not equipped with built-in depth sensor posing additional challenges. The main blocks of the algorithm are: 
  (i) hand segmentaion, (ii) stable hand detection, (iii) left/right hand detection (iv) thumb and index finger segmentation 
  (v) tracking & analysing thumb & index finger movement. First we average out similar texture region and highlight edges between different texture. Skin pixel detection followed by largest contour segmentation gives the hand region in the user's FOV. Left/Right hand detection is done by determing the slope of hand, because hand orientation will be of fix type in Google Cardboard/HMD.
  If Left hand is detected, right most point on hand countour is extreme point. Similarly, if right hand is detected, left most point on 
  hand countour is extreme point. After the left/right hand is detected, center of extreme point & centroid of contour is calculated.
  The hand region towards centroid of contour from center is removed. Now of the largest two blob one is thumb & other is index
  finger. The distance between fingres is analysed to classify Zoom gesture. We conducted experiments which  demonstrates  that our method of Zoom 
  Gesture, work in real time, on device in complex background. </p>

<h3>
  <h4>
<a id="Hand Segmentation" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

<p><img src="https://github.com/ondevicezoomGesture/zOOmgesture/blob/master/img/flowdiagramnew.jpg?raw=true?" alt="Proposed Method " height = "600" width="550" align = "middle"></p>

<p align="justify"> We  present an  approach  for  marker-less  and  real-time touch-less gestures for Zoom in/out on wearables in FPV by tracking
  the movement of Thumb and Index finger. Our  approach  is  particularly  suitable  as  most  of  the smartphones available in 
  the market are not equipped with built-in depth sensor posing additional challenges. The main blocks of the algorithm are: 
  (i) hand segmentaion, (ii) stable hand detection, (iii) left/right hand detection (iv) thumb and index finger segmentation 
  (v) tracking & analysing thumb & index finger movement. First we average out similar texture region and highlight edges between different texture. Skin pixel detection followed by largest contour segmentation gives the hand region in the user's FOV. Left/Right hand detection is done by determing the slope of hand, because hand orientation will be of fix type in Google Cardboard/HMD.
  If Left hand is detected, right most point on hand countour is extreme point. Similarly, if right hand is detected, left most point on 
  hand countour is extreme point. After the left/right hand is detected, center of extreme point & centroid of contour is calculated.
  The hand region towards centroid of contour from center is removed. Now of the largest two blob one is thumb & other is index
  finger. The distance between fingres is analysed to classify Zoom gesture. We conducted experiments which  demonstrates  that our method of Zoom 
  Gesture, work in real time, on device in complex background. </p>

<h4>
  
  
  
  
  
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/oo_dkjwVLlA" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/ondevicezoomGesture/zOOmgesture">zoom Gesture</a> is maintained by <a href="https://github.com/ondevicezoomGesture">ondevicezoomGesture</a>.</span>

      </footer>

    </section>

  </body>
</html>
